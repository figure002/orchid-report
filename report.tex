\documentclass[3p,twocolumn,10pt]{elsarticle}
\setlength{\footskip}{30pt}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{tgpagella}

% Extensive support for hypertext.
\usepackage[hidelinks,colorlinks=true]{hyperref}

% Named references.
\usepackage{nameref}

% Easy access to the Lorem Ipsum dummy text.
\usepackage{lipsum}

% Pro­vides both fore­ground (text, rules, etc.) and back­ground colour man­age­ment.
\usepackage{color}

% Driver-independent color extensions.
\usepackage[x11names]{xcolor}

% Enhanced support for graphics.
\usepackage{graphicx}

% Produces figures which text can flow around.
\usepackage{wrapfig}

% Customising captions in floating environments.
\usepackage{caption}

% Publication quality tables.
\usepackage{booktabs}

% Long tables.
\usepackage{longtable}

%% The amssymb package provides various useful mathematical symbols.
%\usepackage{amssymb}

%% The amsthm package provides extended theorem environments.
%\usepackage{amsthm}

\journal{Journal Name}

% Configure caption display.
\captionsetup{margin=10pt,font=small,labelfont=bf,labelsep=period}

% Define where the images can be found.
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\graphicspath{{./images/}}

% Define some commands.
\input{db-stats.inc}

\begin{document}

% Configure hyperlink colors after document start to override possible
% documentclass defaults.
\hypersetup{
    citecolor=DodgerBlue4,
    filecolor=black,
    linkcolor=black,
    urlcolor=DodgerBlue4
}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Automated identification of slipper orchids using image analysis and artificial neural networks}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}


\author[nbc]{Serrano Pereira}
\author[nbc,hsl,lu]{Barbara Gravendeel}
\author[hsl]{Patrick Wijntjes}
\author[nbc]{Rutger Vos}
\address[nbc]{Naturalis Biodiversity Center, Leiden, The Netherlands}
\address[hsl]{University of Applied Sciences Leiden, Leiden, The Netherlands}
\address[lu]{Institute Biology Leiden, Leiden University, Leiden, The Netherlands}

% Dave Roberts co-author?

\begin{abstract}
A generic hierarchical identification system was developed for the automated identification of species by image recognition. The effectiveness of this system was assessed using photographs of orchids of the subfamily Cypripedioideae. Colour and shape features were extracted from single flower photos of {\SpeciesCount} orchid species. Generic image preprocessing, segmentation, and feature extraction algorithms were implemented to obtain morphometric data for the different orchid species. The identification system uses a hierarchy of artificial neural networks for pattern recognition and automated classification. The neural network hierarchy mirrors the taxonomic hierarchy of the Cypripedioideae, such that user-submitted photos could be assigned a genus, section, and species classification. The ability of the identification system to correctly identify user-submitted photos varied depending on the photo quality, the number of species included for training, and the desired taxonomic level for identification. High quality photos were scarce for some taxa and were under-represented in the training set, resulting in imbalanced network training. The simple colour features used for training were not sufficient to reliably identify photos to the correct section and species, and more specialised feature extraction algorithms need to be developed to improve accuracy. The outcomes of this project include an open source library of feature extraction libraries called ImgPheno, a collection of scripts for neural network training called NBClassify, and a web-interface for NBClassify called OrchiD for identification of user-submitted images.
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

Cypripedioideae \sep feature extraction \sep identification \sep image analysis
\sep neural networks \sep orchids \sep pattern recognition \sep taxonomy

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:introduction}

Correct taxonomic identification of life is for different reasons of great importance. Considering the case of the Cypripedioideae, a horticultural popular group of orchids species and from which many species are highly endangered in the wild, correct taxonomic identification is necessary in order to protect them. Expert taxonomic identification is rare and costly, so alternative methods for taxonomic identification that are both cheap and accurate enough would be of great value. Ongoing advances in computer vision and machine learning has lead to the development of numerous semi- and fully automated species identification systems. Such systems have been successful in the identification of plant species \citep{Arinkin2014, Nilsback2008, Sanz2013}, phytoplankton \citep{Boddy1994}, diatom frustules \citep{Kloster2014}, and insects \citep{Weeks1999,Kang2012}, amongst others. Artificial Neural Networks (ANNs) in particular have become an increasingly popular choice in automated image classification systems \citep{Weeks1997}. They provide a powerful tool for pattern recognition and machine learning.

\section{Materials and methods}
\label{sec:methods}

% Used social media (crowd sourcing) for the collection of images.

\subsection{Reference photo collection}

In order to enable orchid identification through artificial neural networks means that neural networks must first be trained with a sufficient amount of photographs. A collection of reference photos was compiled by searching the internet for images, mostly through Google Image searches. Images were obtained from several multimedia resources like Flickr (\url{https://www.flickr.com}) and Wikimedia Commons (\url{http://commons.wikimedia.org}), but also from several orchid related websites (see \nameref{sec:acknowledgements}).

Simple strategies are needed for maintaining a large collection of reference images and their classification labels. An ideal solution would be a central server where images are stored along with their meta-data (i.e. taxonomic information required for training the neural networks). Such a database needs to be accessible via an application programming interface (API) which would allow for the implementation of custom image harvesters for easy retrieval of images and their meta-data. Secondly, such a database would need to be accessible from the internet so that images and meta-data could be maintained collaboratively via the internet.

The online image hosting service Flickr was used since it meets these requirements. Every photo was uploaded to an account on Flickr, where each image was annotated with standardized tags in the formats \verb/genus:name/, \verb/section:name/, and \verb/species:name/. The correct genus, section, and species for each image was verified by specialists and referenced to the literature \citet{Cribb1998, Pridgeon1999, Frosch2012} and phylogenetic reconstructions based on molecular analyses \citep{Li2011, Chochai2012}. A custom Python script utilising the Flickr API was implemented to mirror the reference images and associated meta-data on a local hard-drive prior image analysis. Images are stored in a directory hierarchy such that the directory names correspond to the image classes. Images are placed in subdirectories to correspond to their phylogenetic classifications. A local meta-data database (\ref{sec:meta-database}) is created for each reference image directory to enable querying of contained images, which is needed in downstream image processing.

\subsection{Image feature extraction}

Since raw pixel data are usually not appropriate for pattern recognition algorithms, images need to be summarised to one or more numerical values. These values, often called features, describe some aspect of the image contents. The algorithms or applications that compute these features are often called feature descriptors or feature extractors. Many feature extraction algorithms are generic and are not limited to specific images. Feature extractors can be applied to entire images and sometimes to regions of interest (ROI) in images. Many feature extraction algorithms exist and are described in the literature, and some have been implemented in software or software libraries for use in other software. Image recognition software that use pattern recognition and machine learning usually implement one or more feature extraction algorithms to summarize features and use those to train classifiers. With so many different feature extraction algorithms in the wild and new ones emerging periodically, finding the right algorithm for a specific image processing problem can be a daunting task, let alone having to implement such algorithms yourself if no existing implementation is readily available. Open Source Computer Vision (OpenCV) and Open Intelligent Multimedia Analysis for Java (OpenIMAJ) are comprehensive computer vision libraries that come with computer vision and machine learning algorithms (amongst others), but support for feature descriptors is currently limited. One of our goals was to implement a comprehensive and open-source software library of image feature description algorithms for the Python programming language, similar to what JFeatureLib does for Java. A proof-of-concept Python package called ImgPheno (\url{https://github.com/naturalis/imgpheno}) was developed during this study, which is not at all comprehensive in its current early development stage.

ImgPheno was implemented by a collection of proof-of-concept Python scripts (\url{https://github.com/naturalis/nbclassify}) for testing image recognition using hierarchical training of artificial neural networks. The scripts make extensive use of OpenCV \citep{Pulli2012} for computer vision, NumPy \citep{VanderWalt2011} for array manipulation, and of scikit-learn \citep{Pedregosa2011} for data transformation and cross-validation. Feature extraction and neural network training was automated with a custom Python script found in the NBClassify package. Configurations are kept in a separate YAML file so that neural network training can easily be reproduced. Different aspects of the feature extraction process can be configured with this configuration file: input/output format, data scaling, colour correction, foreground segmentation, feature descriptors to use, and a classification hierarchy definition for hierarchical training.

As part of the feature extraction workflow, each image is first scaled down if the image exceeds a predefined maximum dimension. This is followed by foreground segmentation to get the region of interest (ROI) matching the flower. Foreground segmentation is done with an iterative approach using the GrabCut segmentation algorithm \citep{Rother2004}. The ROI for the first iteration is set to the entire image and is shrank a fixed number of pixels to get a margin around the ROI containing obvious background pixels (figure \ref{fig:grabcut-output}). Images must therefore be reasonably standardized such that the flower is within this initial ROI. There must also be good contrast between the flower and the background, as to keep segmentation errors to a minimum. Subsequent iterations further classify pixels inside the ROI as foreground or background until the maximum number of iterations is reached. Contours are then obtained from the resulting binary mask, a copy of the image where foreground pixels are made white and background pixels black. Sometimes multiple contours are found (e.g. on images with multiple flowers) in which case only the largest contour is used to select the foreground pixels for downstream feature extraction.

\begin{figure*}[t]
    \centering
    \minipage{\textwidth}
        \includegraphics[width=0.495\linewidth]{grabcut_output_roi.png}
        \includegraphics[width=0.495\linewidth]{grabcut_output.png}
    \endminipage
    \caption{Foreground segmentation with the GrabCut algorithm. The image on the left displays how the initial region of interest is set during automated feature extraction. Photo of \textit{Paphiopedilum druryi} by Peter Tremain.}
    \label{fig:grabcut-output}
\end{figure*}

Only one feature extractor was used to extract features for subsequent training of the artificial neural networks. This feature extractor computes summarised histograms from the pixel colour intensities for the BGR colour space. This was done by calculating the up-right bounding square for the main contour, dividing the square into $N$ equal horizontal and vertical sections (bins), and calculating the mean blue, green, and red colour intensity for each bin. This results in two histograms, one for the horizontal and one for the vertical bins (figure~\ref{fig:bgr-means-sections}). Plotting the histograms shows that this feature captures some aspect of flower shape as well (figure~\ref{fig:bgr-means-plots}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\textwidth]{bgr_means_sections.png}
    \caption{The second horizontal and vertical bin ($N~=~20$) is highlighted in a foreground segmented image of \textit{P.~druryi}.}
    \label{fig:bgr-means-sections}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{bgr_means_plots.pdf}
    \caption{Plots of the mean BGR colour intensities for an image of \textit{P. druryi}. The plots display the mean intensities for the horizontal and vertical bins respectively.}
    \label{fig:bgr-means-plots}
\end{figure*}

\subsection{Neural network training}

Because the 140+ species of the subfamily Cypripedioideae poses a challenge for training a single neural network, a hierarchical approach was used. Hierarchical multi-species classification was done with a hierarchical system of feedforward artificial neural networks that was trained using the iRPROP backpropagation training algorithm. Artificial neural network (ANN) training was implemented with the Fast Artificial Neural Network Library (FANN) by \citet{Nissen2003}. Hierarchical training results in multiple neural networks: one ANN for genus classification, one ANN per genus for section classification, and one ANN per section for species classification. For the individual neural networks this means they need to differentiate between fewer classes and they can be trained using rank or taxon specific features. No ANNs need to be trained on nodes where no branching occurs in the hierarchy (e.g. unispecific genera). Hierarchical classification also means that an incorrect classification at a higher rank in the hierarchy results in incorrect subsequent classifications at lower ranks. It also allows for partial classifications when classification fails for the lower ranks.

The classification hierarchy is defined in the configurations file and is defined as a list of levels for the classification hierarchy which tells the training script the path to follow during training and classification. In this study the levels were named after the corresponding taxonomic ranks: $genus \rightarrow section \rightarrow species$. Different features and training parameters can be set at each level in the classification hierarchy, which the script then uses to generate training data for the required ANNs. All training data was scaled to a -1 to 1 range as per the span of the used activation function (symmetric sigmoid). Codewords were used to encode the class for each sample in the training data. Each ANN has its own set of codewords generated from the sorted lists of classes the ANN is trained on. Each codeword consists of $N$ bits, where $N$ is the number of classes the ANN is trained on. All bits in the codeword are set to an off value (-1), except for one bit (1) which corresponds to the class in the sorted list of classes. Once the training data is generated they are used to train the individual neural networks.

FANN supports many training parameters (e.g. training algorithms, activation functions, neural network topology, etc.) and since there is no way of telling which parameters and which combination of algorithms work best for a specific classification problem, tuning of parameters becomes a matter of trial-and-error. A framework for evolving optimal neural networks (\verb/AI::FANN::Evolving/, \url{https://github.com/naturalis/ai-fann-evolving}) was implemented to overcome this issue. The library implements a genetic algorithm to automate the trial-and-error process with different training parameters, which results in a single ``fittest'' neural network.

\subsection{Cross-validation}

Stratified k-folds cross-validation was performed to estimate the accuracy of hierarchical classification. Genus-section-species combinations were used as the classes for the cross-validations. Because the amount of images for some taxa were very limited, taxa for which the number of images was less than the number of folds ($k$) were excluded. Cross-validation was performed with both standard training and training via the genetic algorithm.

Principal components analysis (PCA) was performed on the training data to assess the suitability of the features for differentiating the photos by genus, section, and species. PCA with orthogonal rotation (varimax) was conducted on several training data sets containing -1 to 1 scaled mean colour intensities for the BGR colour space with 20 horizontal and vertical bins, which translates to 120 features. The Kaiser-Meyer-Olkin measure was used to verify the sampling adequacy for the analysis. Bartlett's test of sphericity was used to get indications whether correlations between items were sufficiently large for PCA. The inflexion in scree plots were used to set the number of components to be extracted. PCA was performed on training data for genus classification, section classification within the genus \textit{Paphiopedilum}, and for species classification within the section \textit{Parvisepalum} of genus \textit{Paphiopedilum}. PCA was not performed for the remaining data sets because the number of features often exceeds the sample size.

\section{Results}
\label{sec:results}

\subsection{Reference photo collection}

In total {\PhotoCount} photos for {\SpeciesCount} species were collected from various sources (table~\ref{tbl:photo-counts}). The collection represents all five genera from subfamily Cypripedioideae: \textit{Cypripedium}, \textit{Mexipedium}, \textit{Paphiopedilum}, \textit{Phragmipedium}, and \textit{Selenipedium}. With the number of photos for the five genera ranging from just 4 for the genus \textit{Selenipedium} to 888 for \textit{Paphiopedilum}, the collection is highly unbalanced. There is also a high variation in the number of images within genera. Unbalanced data results in a bias towards the more image rich taxa during neural network training, meaning the under-represented taxa are less likely to be classified correctly.

Because images of some taxa were hard to find, we had to make some compromises with respect to the quality of the images. Many of the gathered images are not of the desired standardised format, resulting in slight variations in lighting, dimension, and flower position or rotation, which makes pattern recognition even more challenging.

\begin{table}[h]\footnotesize
    \caption{The number of photos collected per genus, as well as the number of sections and species per genus represented by the photo collection.}
    \begin{center}
    \begin{tabular}{llll}
    \toprule
    \textbf{Genus} & \textbf{Section} & \textbf{Species} & \textbf{Photos} \\
    \midrule
    \input{table-taxa-summary.inc}
    \bottomrule
    \end{tabular}
    \end{center}
    \label{tbl:photo-counts}
\end{table}

\subsection{Image feature extraction}

Several image feature extraction methods were implemented in the ImgPheno library. These include a custom algorithm for getting colour and shape properties, as well as methods for obtaining contour properties (e.g. convex area, eccentricity, equivalent diameter, extent, orientation, perimeter, solidity), and some helper functions for visualising data.

\begin{table}[h]\footnotesize
    \caption{Selection of ImgPheno methods.}
    \begin{center}
    \begin{tabular}{lp{4cm}}
    \toprule
    \textbf{Method} & \textbf{Description} \\
    \midrule
    \verb/color_bgr_means/ & Returns the histograms for BGR images along X and Y axis. \\
    \bottomrule
    \end{tabular}
    \end{center}
    \label{tbl:imgpheno-methods}
\end{table}

\subsection{Cross-validation}

Stratified k-folds cross-validation was performed to estimate the accuracy of the classifiers. The accuracy for genus classification was 75\%, but accuracy drops dramatically as section (52\%) and species (48\%) are also included in the classification (table~\ref{tbl:x-validation-results}). With 10 folds and including only those species for which at least 10 photos are collected, results in slightly better accuracies (table~\ref{tbl:x-validation-results}), but this can probably be attributed to the reduced number of sections and species.

\begin{table}[h]\footnotesize
    \caption{Results for stratified k-fold cross-validations. Cross-validation was performed on three taxonomic ranks: genus, section, and species. The results for genus/section and genus/section/species combine the results from their respective ranks.}
    \begin{center}
    \begin{tabular}{lp{1.5cm}p{1.5cm}}
    \toprule
    \textbf{Classification} & \textbf{Accuracy (k=4)} & \textbf{Accuracy (k=10)} \\
    \midrule
    genus                   & 75\%    & 81\% \\
    section                 & 52\%    & 60\% \\
    species                 & 48\%    & 56\% \\
    genus/section           & 41\%    & 49\% \\
    genus/section/species   & 20\%    & 27\% \\
    \bottomrule
    \end{tabular}
    \end{center}
    \label{tbl:x-validation-results}
\end{table}

Figure~\ref{fig:pca-plots} shows the scatter plots for the principal components that were performed on a subset of the training data. The low classification accuracies described in table \ref{tbl:x-validation-results} may be attributed to the poor discrimination ability of the colour based feature. Out of the four plots, only the plot for species classification within the section \textit{Parvisepalum} of genus \textit{Paphiopedilum} showed some clear clusters.

\section{Conclusions and discussion}
\label{sec:conclusion}

In an effort the normalize images prior to feature extraction, some colour image enhancement methods developed by \citet{Naik2003} were implemented. Most images used were already of good quality, and performing hue-preserving linear transformation with maximum contrast often did not result in an enhanced image, and thus colour enhancement using this method did not result in improved identification accuracy. Several feature extraction method were implemented in the ImgPheno package, including methods for obtaining image moments based shape characteristics, methods for getting contour outline characteristics, specialised shape description methods, and methods for obtaining colour data. This study builds on a simple features extraction method for obtaining colour data, \verb/color_bgr_means/.

As opposed to simply using overall colour data as described in this study, image feature extraction methods should also be developed for extracting specific morphological characters to improve accuracy with hierarchical classification. Developing such specialized features extraction methods is not an easy task however. Many different specialized image feature extraction methods have been developed, and it would be desirable to have these combined in a freely available library, as was attempted in this study. And since multiple morphological characters are usually considered during species identification, multiple features or characters can be represented by the feature data. Identification accuracy can further be improved by means of neural network ensembles, the usage of multiple neural networks to make classifications \citep{Hansen1990}. The identification system presented here does use multiple neural networks for the identification of an image, but these cannot be considered neural networks ensembles because only one neural network is used for each level within the hierarchical classification (i.e. one neural network is used for genus, section, and species separately). The output codes used for training the neural networks as implemented in this study is another aspect that could be improved. The implementation presented here uses the simplest possible output codes for training, but the usage of more sophisticated error codes could improve neural network training and therefore identification accuracy \citep{Dietterich1995}.

% Need more and better photo material.
% Better feature extraction methods.

\section*{Acknowledgements}
\label{sec:acknowledgements}

% De namen van alle fotografen in de format J.H. Simpson.

This research was made possible by the xxx grant. We are thankful to the following photographers and institutions for allowing us to use their photographs for this research: Alpine-Garden.com, Clark Riley, Crustacare, Dalton Holland Baptista, Daniel Winkler, Danuta Panczyk, David Llewelyn, Dot Potter Barnett, Eleanor S. Saulys (Connecticut Orchid Society), Eric Hunt, Eric Hunt, Fren Mah, GoreOrchids.com, Greentours, Holger Perner, J.J. Harrison, Jason Chang, Jean Claessens, John Marcotte, Joseph W. Dougherty, Karma Forester, Markus Bürki (Botanischer Garten Bern), Martin Günther, Mauro Rosim, Mihai Costea, Nile and Lois Dusdieker, Orchi, Patrick Mannens, Peter Tremain, Philipp J. Cribb and the Swiss Orchid Foundation, Pieter C. Brouwer, Pilar Quintana, Ray Barkalow (First Rays Orchids), Rene Klinge, Rick H., Richard Labbe (Planteck), Roberto Takase, Rogier van Vugt, Ron Parsons, Ross Hella, Vermont Ladyslipper Company, Yijia Wang, GoreOrchids.com, and Orchids.com.

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num}
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\section*{References}

%% APA style with "Author, year" format.
\bibliographystyle{apa}
\biboptions{authoryear}

\bibliography{references}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\onecolumn

\section{Metadata database}
\label{sec:meta-database}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{meta_database_diagram.pdf}
    \caption{Diagram of the metadata database used for storing taxonomic information for a collection of reference photos.}
    \label{fig:meta-database}
\end{figure*}

\section{Reference photo collection}
\label{sec:reference-photo-collection}

\begin{scriptsize}
\begin{longtable}{llllll}
    \caption{Taxa represented by the reference photo collection and the number of photos for each species.}
    \label{tbl:taxa-stats}
    \endfirsthead
        \caption*{\textbf{Table \ref{tbl:taxa-stats}.} (continued)}
        \\\textbf{Taxa} & \textbf{Photos} \\
        \midrule
    \endhead
        \midrule
        \caption*{\scriptsize\textit{(continued on next page)}}
    \endfoot
        \bottomrule
    \endlastfoot

    \toprule
    \textbf{Taxa} & \textbf{Photos} \\
    \midrule
    \input{table-taxa.inc}
\end{longtable}
\end{scriptsize}

\clearpage
\section{Scatter plots for principal components analyses}
\label{sec:pca-plots}

\begin{figure}[!h]
    \minipage{\textwidth}
        \includegraphics[width=0.5\linewidth]{genus_pca_plot.pdf}
        \includegraphics[width=0.5\linewidth]{Cypripedium_section_pca_plot.pdf}
    \endminipage
    \par\vfill
    \minipage{\textwidth}
        \includegraphics[width=0.5\linewidth]{Paphiopedilum_section_pca_plot.pdf}
        \includegraphics[width=0.5\linewidth]{Paphiopedilum_Parvisepalum_species_pca_plot.pdf}
    \endminipage
    \caption{Scatter plots for the principal components analysis results. PCA was conducted on training data for genus classification, \textit{Cypripedium} section classification, \textit{Paphiopedilum} section classification, and \textit{Paphiopedilum} section \textit{Parvisepalum} species classification. For all plots the first principal component (PC1) was plotted against the second (PC2).}
    \label{fig:pca-plots}
\end{figure}

\end{document}
